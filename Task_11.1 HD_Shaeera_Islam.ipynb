{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "789c02cb-15e6-4fef-b549-63ca82049e2c",
   "metadata": {},
   "source": [
    "# Part - 1 : Reproduction of published results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb572db3-b458-4716-a718-8c5163649659",
   "metadata": {},
   "source": [
    "Here, we run the individual models along with the paper's proposed 5-fold stacking ensemble model and view their result in different metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "57ad523c-7717-4a07-a3a1-51ae17887e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.840183</td>\n",
       "      <td>0.876190</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.904286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.834146</td>\n",
       "      <td>0.849558</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.793388</td>\n",
       "      <td>0.928000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.855769</td>\n",
       "      <td>0.847619</td>\n",
       "      <td>0.864078</td>\n",
       "      <td>0.958952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.985366</td>\n",
       "      <td>0.985507</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Stacking Ensemble</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  F1 Score    Recall  Precision       AUC\n",
       "0          Naive Bayes  0.829268  0.840183  0.876190   0.807018  0.904286\n",
       "1  Logistic Regression  0.834146  0.849558  0.914286   0.793388  0.928000\n",
       "2                  KNN  0.853659  0.855769  0.847619   0.864078  0.958952\n",
       "3        Decision Tree  0.985366  0.985507  0.971429   1.000000  0.985714\n",
       "4        Random Forest  1.000000  1.000000  1.000000   1.000000  1.000000\n",
       "5              XGBoost  1.000000  1.000000  1.000000   1.000000  1.000000\n",
       "6    Stacking Ensemble  1.000000  1.000000  1.000000   1.000000  1.000000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Dataset\n",
    "df = pd.read_csv(\"D:/Deakin Coursework/T1 semester/SIT - 720 - Machine Learning/Task 11.1HD/11.1HD/heart.csv\")\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop(columns=\"target\")\n",
    "y = df[\"target\"]\n",
    "\n",
    "# Preprocessing\n",
    "scaler = MinMaxScaler()          # Scale features using MinMaxScaler\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Defining models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss', random_state=42),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"KNN\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Evaluate and collect results\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"F1 Score\": f1_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"AUC\": roc_auc_score(y_test, y_prob)\n",
    "    })\n",
    "    \n",
    "# Stacking Ensemble from the paper's proposed model\n",
    "stack_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(max_iter=1000)),\n",
    "        ('dt', DecisionTreeClassifier()),\n",
    "        ('rf', RandomForestClassifier(n_estimators=100)),\n",
    "        ('xgb', XGBClassifier(eval_metric='logloss'))\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "stack_model.fit(X_train, y_train)\n",
    "y_pred = stack_model.predict(X_test)\n",
    "y_prob = stack_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "results.append({\n",
    "    \"Model\": \"Stacking Ensemble\",\n",
    "    \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "    \"F1 Score\": f1_score(y_test, y_pred),\n",
    "    \"Recall\": recall_score(y_test, y_pred),\n",
    "    \"Precision\": precision_score(y_test, y_pred),\n",
    "    \"AUC\": roc_auc_score(y_test, y_prob)\n",
    "})\n",
    "# Results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.sort_values(by=\"Accuracy\", ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4cab24-9d8a-4d51-98fb-db10b600925f",
   "metadata": {},
   "source": [
    "The results are slightly different than the proposed paper's. This could be due to the fact that although paper used train/test split, the exact ratio is not specified. We used 80/20 split as standard. The parameters for logistic regression, decision tree, random forest and KNN is not mentioned. So we can only assume them or use default. In our reproduction of their report's value, we used max_iter = 1000 for Logistic Regression and n_estimators=100 for Random Forest and default for others. \n",
    "Their proposed model itself was not perfect 1.00 or 100%, however our version of their model was. We applied 5-fold stacking ensemble like the paper had done. However our results gave better result. This could be due to the train/test split decision and using different parameters. Hence the difference in results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a1effc-fe81-4091-930c-47100e6d7528",
   "metadata": {},
   "source": [
    "# Part - 2: Creation of our own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "249b0e99-3c4f-4065-9f14-6aa196bef285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our proposed model:\n",
      "\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "AUC: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Reloading the libraries and dataset to keep my model seprerated overall from the workings of the reproduction of the paper' model.\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"D:/Deakin Coursework/T1 semester/SIT - 720 - Machine Learning/Task 11.1HD/11.1HD/heart.csv\")\n",
    "X = df.drop(columns=\"target\")\n",
    "y = df[\"target\"]\n",
    "\n",
    "# Scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# RFE Feature Selection\n",
    "rfe_selector = RFE(estimator=LogisticRegression(max_iter=1000), n_features_to_select=10)\n",
    "X_rfe = rfe_selector.fit_transform(X_scaled, y)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_rfe, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Applying SMOTE to balance training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Defining base models\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "xgb = XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "\n",
    "# Voting ensemble\n",
    "voting = VotingClassifier(estimators=[('lr', lr), ('rf', rf), ('xgb', xgb)], voting='soft')\n",
    "voting.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = voting.predict(X_test)\n",
    "y_prob = voting.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "    \"Precision\": precision_score(y_test, y_pred),\n",
    "    \"Recall\": recall_score(y_test, y_pred),\n",
    "    \"F1 Score\": f1_score(y_test, y_pred),\n",
    "    \"AUC\": roc_auc_score(y_test, y_prob)\n",
    "}\n",
    "\n",
    "# Results\n",
    "print(\"\\nOur proposed model:\\n\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
